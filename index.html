<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiaoyun Yuan</title>
  
  <meta name="author" content="Xiaoyun Yuan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="imgs/th_icon_s.png">
  <style type="text/css">body {zoom:1.15;}</style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiaoyun Yuan, Ph.D.</name>
              </p>
              <p>I am a postdoctoral researcher (Supervisor: <a href="http://luvision.net">Dr. Lu Fang</a>) at <a href="http://luvision.net">Sigma lab</a>, Tsinghua University</a>,  where I work on Optical Intelligent Computing and Computational Photography.
              </p>
              <p>Updated Oct 27, 2023</p>
              
              <p>
                Ph.D., HKUST 2020  &nbsp&nbsp &nbsp&nbsp  &nbsp&nbsp B.Sc., USTC 2014
              </p>
              <p>
                <a>xyuanag AT connect.ust.hk</a> &nbsp/&nbsp
                <!--<a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=MrEV0uwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://www.linkedin.com/in/xiaoyunyuan/">Linkedin</a> &nbsp/&nbsp -->
                <a href="https://github.com/yuanxy92">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="imgs/Xiaoyun.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="imgs/Xiaoyun.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>

            <p>
              1. Our paper "Training large-scale optoelectronic neural networks  with dual-neuron optical-artificial learning" was accepted by <b>Nature Communications</b>. <em>Sept 25, 2023. </em>
            </p>

            <p>
              2. We are holding <a href="https://gigavision.cn/">GigaVision challenges </a>, with a total prize of <b>USD 400,000 (CNY 3,000,000)</b>. GigaVision program seeks to revolutionize computer vision when it meets gigapixel videography with both wide field-of-view and high-resolution details. Welcome!!! <em>Sept 1, 2022</em>
            </p>

            <p>
              3. Our paper <a href="https://www.nature.com/articles/s41377-022-00945-y"> "A multichannel optical computing architecture for advanced machine vision"</a> was accepted by Light: Science & Applications. <em>Aug 18, 2022</em>
            </p>
          
          </td>
        </tr>
      </tbody></table>

        <!-- Research -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected publications</heading>
              <p>
                I'm interested in Gigapixel Imaging, Optical Computing and Photoacoustic Imaging.
                Representative papers are listed.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/nc_DANTE.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>Training large-scale optoelectronic neural networks with dual-neuron optical-artificial learning</papertitle>
              </a>
              <br>
              <b>Xiaoyun Yuan</b>, Yong Wang, Zhihao Xu, Tiankuang Zhou, Lu Fang
              <br>
              <b><em>Nature Communications</em></b>, 2023  
              <br>
              <a href="https://www.nature.com/articles/s41467-023-42984-y">Paper</a>&nbsp&nbsp&nbsp&nbsp&nbsp
              <a href="https://github.com/yuanxy92/DANTE">Code</a>  
              <p>We present DANTE, a dual-neuron optical-artificial learning architecture. Optical neurons model the optical diffraction, while artificial neurons approximate the intensive optical-diffraction computations with lightweight functions. DANTE also improves convergence by employing iterative global artificial-learning steps and local optical-learning steps. In simulation experiments, DANTE successfully trains large-scale ONNs with 150 million neurons on ImageNet, previously unattainable, and accelerates training speeds significantly on the CIFAR-10 benchmark compared to single-neuron learning. In physical experiments, we develop a two-layer ONN system based on DANTE, which can effectively extract features to improve the classification of natural images.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/lsa_monet.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>A multichannel optical computing architecture for advanced machine vision (Editors' Hightlight)</papertitle>
              </a>
              <br>
              Zhihao Xu*, <b>Xiaoyun Yuan*</b>, Tiankuang Zhou, Lu Fang
              <br>
              <b><em>Light: Science and Applications</em></b>, 2022  
              <br>
              <a href="https://www.nature.com/articles/s41377-022-00945-y">Paper</a> 
              <p>Herein, we develop Monet: a multichannel optical neural network architecture for a universal multiple-input multiple-channel optical computing based on a novel projection-interference-prediction framework where the inter- and intra- channel connections are mapped to optical interference and diffraction. For the first time, Monet validates that multichannel processing properties can be optically implemented with high-efficiency, enabling real-world intelligent multichannel-processing tasks solved via optical computing, including 3D/motion detections. 
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/tpami_gigamvs.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>GigaMVS: A Benchmark for Ultra-large-scale Gigapixel-level 3D Reconstruction
                </papertitle>
              </a>
              <br>
              Jianing Zhang*, Jinzhi Zhang*, Shi Mao*, Mengqi Ji, Guangyu Wang, Zequn Chen, Tian Zhang, <b>Xiaoyun Yuan</b>, Qionghai Dai, Lu Fang 
              <br>
              <b><em>IEEE Transactions on Pattern Analysis & Machine Intelligence
              </em></b>, 2021  
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9547729/">Paper</a> 
              <p>Multiview stereopsis (MVS) methods, which can reconstruct both the 3D geometry and texture from multiple images, have been rapidly developed and extensively investigated from the feature engineering methods to the data-driven ones. However, there is no dataset containing both the 3D geometry of large-scale scenes and high-resolution observations of small details to benchmark the algorithms. To this end, we present GigaMVS, the first gigapixel-image-based 3D reconstruction benchmark for ultra-large-scale scenes. ...
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/lsa_giga.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>A modular hierarchical array camera (cover article)</papertitle>
              </a>
              <br>
              <b>Xiaoyun Yuan*</b>, Mengqi Ji*, Jiamin Wu, David J. Brady, Qionghai Dai, Lu Fang
              <br>
              <b><em>Light: Science and Applications</em></b>, 2021  
              <br>
              <a href="https://www.nature.com/articles/s41377-021-00485-x">Paper</a> 
              <p>We develop an unstructured array camera system that adopts a hierarchical modular design with multiscale hybrid cameras composing different modules. Intelligent computations are designed to collaboratively operate along both intra- and intermodule pathways. This system can adaptively allocate imagery resources to dramatically reduce the hardware cost and possesses unprecedented flexibility, robustness, and versatility.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/nbme_pa.webp' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>Massively parallel functional photoacoustic computed tomography of the human brain
                </papertitle>
              </a>
              <br>
              Shuai Na*, Jonathan J. Russin*, Li Lin*, <b>Xiaoyun Yuan*</b>, Peng Hu, Kay B. Jann, Lirong Yan, Konstantin Maslov, Junhui Shi, Danny J. Wang, Charles Y. Liu, Lihong V. Wang 
              <br>
              <b><em>Nature Biomedical Engineering</em></b>, 2021  
              <br>
              <a href="https://www.nature.com/articles/s41551-021-00735-8">Paper</a> 
              <p>Here, we show that massively parallel ultrasonic transducers arranged hemispherically around the human head can produce tomographic images of the brain with a 10-cm-diameter FOV and spatial and temporal resolutions of 350 µm and 2 s, respectively. Our findings establish the use of photoacoustic computed tomography for human brain imaging.
              </p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/multiscale_vr.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>Multiscale-VR: Multiscale Gigapixel 3D Panoramic Videography for Virtual Reality</papertitle>
              </a>
              <br>
              Jianing Zhang*, Tianyi Zhu*, Anke Zhang*, <b>Xiaoyun Yuan*</b>, Zihan Wang, Sebastian Beetschen, Lan Xu, Xing Lin, Qionghai Dai, Lu Fang
              <br>
              <b><em>IEEE International Conference on Computational Photography (ICCP)</em></b>, 2020
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9105244">Paper</a>
              <p>In this work, we propose Multiscale-VR, a multiscale unstructured camera array computational imaging system for high-quality gigapixel 3D panoramic videography that creates the six-degree-of-freedom multiscale interactive VR content. The Multiscale-VR imaging system comprises scalable cylindrical-distributed global and local cameras, where global stereo cameras are stitched to cover 360° field-of-view, and unstructured local monocular cameras are adapted to the global camera for flexible high-resolution video streaming arrangement. 
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/nc_pa.webp' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>High-speed three-dimensional photoacoustic computed tomography for preclinical research and clinical translation</papertitle>
              </a>
              <br>
              Li Lin*, Peng Hu*, Xin Tong*, Shuai Na*, Rui Cao, <b>Xiaoyun Yuan</b>, David C Garrett, Junhui Shi, Konstantin Maslov, Lihong V Wang
              <br>
              <b><em>Nature Communications</em></b>, 2021  
              <br>
              <a href="https://www.nature.com/articles/s41467-021-21232-1">Paper</a> 
              <p>We developed a three-dimensional photoacoustic computed tomography (3D-PACT) system that features large imaging depth, scalable field of view with isotropic spatial resolution, high imaging speed, and superior image quality.
              </p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/ICCP_giga_2017.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>Multiscale gigapixel video: A cross resolution image matching and warping approach</papertitle>
              </a>
              <br>
              <b>Xiaoyun Yuan</b>, Lu Fang, Qionghai Dai, David J Brady, Yebin Liu
              <br>
              <b><em>IEEE International Conference on Computational Photography (ICCP)</em></b>, 2017
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/7951481">Paper</a>
              <p>We present a multi-scale camera array to capture and synthesize gigapixel videos in an efficient way. Our acquisition setup contains a reference camera with a short-focus lens to get a large field-of-view video and a number of unstructured long-focus cameras to capture local-view details. 
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/TCSVT_glasses.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>Magic glasses: from 2D to 3D</papertitle>
              </a>
              <br>
              <b>Xiaoyun Yuan*</b>, Difei Tang*, Yebin Liu, Qing Ling, Lu Fang
              <br>
              <b><em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) </em></b>, 2016
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/7457667">Paper</a>
              <p>This paper proposes a virtual 3D eyeglasses try-on system driven by a 2D Internet image of a human face wearing with a pair of eyeglasses. The main technical challenge of this system is the automatic 3D eyeglasses model reconstruction from the 2D glasses on a frontal human face. 
              </p>
            </td>
          </tr> 
        

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/crossnet.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>Crossnet++: Cross-scale large-parallax warping for reference-based super-resolution</papertitle>
              </a>
              <br>
              Yang Tan*, Haitian Zheng*, Yinheng Zhu, <b>Xiaoyun Yuan</b>, Xing Lin, David Brady, Lu Fang
              <br>
              <b><em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em></b>, 2020
              <br>
              <a href="https://www.computer.org/csdl/journal/tp/5555/01/09099445/1k7oyvQ9LzO">Paper</a>
              <p>We present CrossNet++, an end-to-end network containing novel two-stage cross-scale warping modules. The stage I learns to narrow down the parallax distinctively with the strong guidance of landmarks and intensity distribution consensus. Then the stage II operates more fine-grained alignment and aggregation in feature domain to synthesize the final super-resolved image. To further address the large parallax, new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss are proposed to regularize training and enable better convergence.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='imgs/panda.jpg' width="320">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a>
                <papertitle>Panda: A gigapixel-level human-centric video dataset</papertitle>
              </a>
              <br>
              Xueyang Wang*, Xiya Zhang*, Yinheng Zhu*, Yuchen Guo*, <b>Xiaoyun Yuan</b>, Liuyu Xiang, Zerun Wang, Guiguang Ding, David Brady, Qionghai Dai, Lu Fang
              <br>
              <b><em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em></b>, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_PANDA_A_Gigapixel-Level_Human-Centric_Video_Dataset_CVPR_2020_paper.html">Paper</a> /
              <a href="http://www.panda-dataset.com/">Project page</a> 
              <p>PANDA is the first gigaPixel-level humAN-centric viDeo dAtaset, for large-scale, long-term, and multi-object visual analysis. The videos in PANDA were captured by a gigapixel camera and cover real-world large-scale scenes with both wide field-of-view (~1km^2 area) and high resolution details (~gigapixel-level/frame). The scenes may contain 4k head counts with over 100× scale variation. PANDA provides enriched and hierarchical ground-truth annotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute labels, 12.7k trajectories, 2.2k groups and 2.9k interactions.
              </p>
            </td>
          </tr> 


        </tbody></table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, CVPR 2021</a>
              <br><br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br><br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        </tbody></table>
      </td>
    </tr>
  </table> -->
  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5g1be4nc2m6&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>


</body>

</html>
